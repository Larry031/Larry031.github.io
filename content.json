{"meta":{"title":"Larry031的博客","subtitle":"一个少年，去天边，继承大海","description":"自动驾驶算法工程师一枚，专注分享一切有趣的技术和玩物","author":"Larry","url":"http://example.com","root":"/"},"pages":[],"posts":[{"title":"谈谈不足转向梯度及其估计","slug":"谈谈不足转向梯度及其估计","date":"2025-03-25T14:44:23.000Z","updated":"2025-03-26T14:55:28.432Z","comments":true,"path":"2025/03/25/谈谈不足转向梯度及其估计/","link":"","permalink":"http://example.com/2025/03/25/%E8%B0%88%E8%B0%88%E4%B8%8D%E8%B6%B3%E8%BD%AC%E5%90%91%E6%A2%AF%E5%BA%A6%E5%8F%8A%E5%85%B6%E4%BC%B0%E8%AE%A1/","excerpt":"","text":"1. 什么是不足转向梯度 首先解释一下什么是不足转向(understeer)。既然有不足转向，那么是不是也存在\"过度转向(oversteer)\"？没错，另外还有\"中性转向\"。 常见汽车测评中的解释是，不足转向即所谓推头，弯道中容易冲出车道。所谓“过度转向”，就是转向太灵活了，弯道中容易侧滑或甩尾。借用下面的图可以较好的理解。 那么，为什么不同的车会有不同的转向特性呢？（接下来内容较为硬核，谨慎食用） 根据汽车动力学中的线性二自由度汽车模型，稳态转向时可推出（具体推导过程可参见Fundamentals of Vehicle Dynamics）： \\[\\delta = \\frac{L}{R} + (\\frac{W_{fs}}{C_f}-\\frac{W_{rs}}{C_r})\\frac{u^2}{g*R} = \\frac{L}{R} + K\\frac{a_y}{g} \\tag{1}\\] \\[K = \\frac{W_{fs}}{C_f}-\\frac{W_{rs}}{C_r}\\] 其中： \\(K\\) - 不足转向梯度(rad/g) \\(\\delta\\)-前轮转向角(r. d) \\(L\\)-轴距(m) \\(R\\)-转向半径(m) \\(u\\)-前进速度(m/s) \\(g\\)-重力加速度 \\(W_{fs}\\)-前. 静态载荷(N) \\(W_{rs}\\)-. 轴静态载荷(N) \\(C_r\\)-两个前轮胎的侧偏刚度（N/rad） \\(C_r\\)-两个后轮胎的侧偏刚度(N/rad) \\(a_y\\) - 侧向加速度 上式可改写为： \\[ R= \\frac{L}{\\delta} + K\\frac{u^2}{\\delta} = \\frac{L+K*u^2}{g\\delta} \\] 1. \\(K = 0\\) 中性转向：转向半径与车速无关，只取决于轴距与转角比值，这意味着只要保持方向盘转角不变，无论车速快慢，运动半径都是不变的。 2. \\(K &gt; 0\\) 不足转向：转向半径随着车速的增加而增大，这意味着如果保持方向盘转角不变，车速越大，运动半径也就越大，这个时候就会冲出弯道，即所谓的转向不足。 3. \\(K &lt; 0\\) 过度转向：转向半径随着车速的增加而减小，这意味着如果保持方向盘转角不变，车速越大，运动半径越小，此时车辆便会向弯道内偏转，容易造成甩尾。 所以，不足转向梯度决定了车辆在转弯时的特性，由车辆参数所决定。 由此，我们便可以很好地理解为什么主打操控的运动型车辆喜欢采用前置后驱或中置后驱 为了操控的准确性（俗称指哪儿打哪儿），其设计倾向于中性转向，即\\(K=0\\)。这就要求前后载荷比为50:50,所以设计成前置后驱或后置后驱来达到前后载荷平衡。 赛车为了追求过弯速度，一般都设计成后置后驱的形式，前后载荷小于后轴载荷，此时\\(K &lt; 0\\) ，具有过度转向特性。 家用车为了保证在弯道中的稳定性，一般设计成前置前驱的不足转向特性，此时前轴载荷大于后轴，\\(K &gt; 0\\) 。 2.为什么要估计不足转向梯度 在自动驾驶横向控制中，一般根据规划轨迹计算出期望方向盘转角，常用控制方法之一的前馈PID要计算一个前馈转角。根据公式（1），我们只需要知道车辆轴距、弯道半径、侧向加速度（这三个参数都比较容易获取）和不足转向梯度就可以计算前馈转角。 不足转向梯度由车辆参数所决定。但有时候我们并不能拿到车辆的准确参数，这时候就需要有参数辨识方法来进行估计。 这里介绍一种递归最小二乘（RLS）估计车辆不足转向梯度的方法。 为什么不用最小二乘法呢？RLS递归最小二乘法(Recursive Least Squares)中给出了很好的解释，通俗易懂，建议全篇背诵！！ 最小二乘法的缺陷： 1. 在实际应用中, 这种直接计算法并不常见, 主要是因为公式中求逆部分\\(A=(X^{\\top}X)^{-1}X^{\\top}\\textbf{y}\\)的计算量大, 在样本数据量大时计算量更是明显增大; 2. 现实生活中,往往出现样本数据可能也并不是一次性给出, 而是不断给出新的样本数据, 以一种数据流的形式给出样本数据。 3. 数据饱和现象 由于我们要做的是不足转向梯度的在线估计，数据样本是以数据流给出的，这时候就需要用递归最小二乘。 递归最小二乘的公式推导在此不再赘述，直接给出： \\[ k = \\frac{PX_{n+1}}{\\lambda+X_{n+1}^{\\top}PX_{n+1}} \\] \\[ P&#39;=\\frac{1}{\\lambda}(P-kX_{n+1}^{\\top}P) \\] \\[ \\mathbf{w&#39;} = \\mathbf{w} + k(y_{n+1}-X_{n+1}\\mathbf{w}) \\] 其中: \\(P\\):逆矩阵. \\(X_{n+1}\\):数据向量. \\(y_{n+1}\\):输出样本. \\(\\lambda\\):遗忘因子. \\(\\mathbf{w}\\):模型参数. 这里，我们通过二自由度模型进行建模： \\[ \\delta = \\frac{L}{R} + K\\frac{a_y}{g} \\] 改写为： \\[ \\delta - \\frac{L}{R}=K\\frac{a_y}{g} = K\\cdot\\frac{wv}{g} \\] 输入为横向加速度\\(a_y\\) . 输出样本为\\(\\delta - \\frac{L}{R}\\) 我们要估计的模型参数\\(K\\)，即. 足转向梯度。 初始值设置： 遗忘因子\\(\\lambda\\)设置为0.999 模型参数\\(\\mathbf{w}\\)不足转向梯度初始值设为2.0（根据车辆类型设置一个一般值,0~3.0） 逆矩阵初始值为\\(1*1\\) 单位矩阵 实际应用中，为了不足专项梯度估计的准确性，应设置触发条件，横向加速度可设为（0.3 ~2.5 m/s^2），速度大于10kph。 下面给出python版本的RLS： 123456789101112131415161718192021222324252627282930313233343536373839404142import numpy as npclass RecursiveLeastSquares: def __init__(self, num_features, lambda_=0.99, delta=1.0): &quot;&quot;&quot; 初始化递归最小二乘法参数 :param num_features: 特征数量 :param lambda_: 遗忘因子 (0 &lt; lambda_ &lt;= 1) :param delta: 初始协方差矩阵的缩放因子 &quot;&quot;&quot; self.num_features = num_features self.lambda_ = lambda_ self.P = np.eye(num_features) * delta # 初始协方差矩阵 self.theta = np.zeros(num_features) # 参数向量 def update(self, x, y): &quot;&quot;&quot; 更新模型参数 :param x: 输入特征向量 (1D array) :param y: 目标值 (标量) &quot;&quot;&quot; x = np.array(x).reshape(-1, 1) # 转为列向量 y = np.array(y).reshape(-1, 1) # 转为列向量 # 计算增益向量 Px = self.P @ x gain = Px / (self.lambda_ + x.T @ Px) # 更新参数向量 error = y - x.T @ self.theta self.theta += (gain * error).flatten() # 更新协方差矩阵 self.P = (self.P - gain @ x.T @ self.P) / self.lambda_ def predict(self, x): &quot;&quot;&quot; 使用当前模型参数进行预测 :param x: 输入特征向量 (1D array) :return: 预测值 (标量) &quot;&quot;&quot; return np.dot(self.theta, x) 使用示例： 12345678910111213141516171819202122232425262728293031323334353637import numpy as npimport matplotlib.pyplot as pltfrom rls import RecursiveLeastSquares# 初始化 RLS 模型rls = RecursiveLeastSquares(num_features=2, lambda_=0.98, delta=10.0)# 模拟数据np.random.seed(0)X = np.random.rand(100, 2) # 输入特征true_theta = np.array([2.0, -3.0]) # 真正的参数y = X @ true_theta + np.random.randn(100) * 0.1 # 目标值# 保存每次迭代的 theta 值theta_history = []# 在线更新模型for i in range(len(X)): rls.update(X[i], y[i]) theta_history.append(rls.theta.copy()) # 保存当前的 theta 值 print(f&quot;Step &#123;i+1&#125;: Estimated theta = &#123;rls.theta&#125;&quot;)# 转换为 numpy 数组以便绘图theta_history = np.array(theta_history)# 绘制参数估计值的变化曲线plt.figure(figsize=(10, 6))for i in range(rls.num_features): plt.plot(theta_history[:, i], label=f&#x27;Theta &#123;i+1&#125;&#x27;)plt.axhline(true_theta[0], color=&#x27;r&#x27;, linestyle=&#x27;--&#x27;, label=&#x27;True Theta 1&#x27;)plt.axhline(true_theta[1], color=&#x27;b&#x27;, linestyle=&#x27;--&#x27;, label=&#x27;True Theta 2&#x27;)plt.xlabel(&#x27;Iteration&#x27;)plt.ylabel(&#x27;Theta Value&#x27;)plt.title(&#x27;Convergence of Theta Estimates&#x27;)plt.legend()plt.grid()plt.show() rls 从上图可以看出，随着迭代次数的增加，模型参数会逐渐收敛。在实际工程应用中，可以设置不足转向梯度为观测量，log后观察估计值是否逐渐收敛。 参考： 1. 在弯道，前驱车转向不足，后驱车转向过度，原理是什么？怎么处理？ 2. Understeer_and_oversteer","categories":[],"tags":[]},{"title":"车道线拟合","slug":"车道线拟合","date":"2024-11-28T15:14:41.000Z","updated":"2024-12-02T14:40:33.357Z","comments":true,"path":"2024/11/28/车道线拟合/","link":"","permalink":"http://example.com/2024/11/28/%E8%BD%A6%E9%81%93%E7%BA%BF%E6%8B%9F%E5%90%88/","excerpt":"","text":"车道线拟合是根据特定数量的采样点拟合出一条多阶的车道线曲线方程。车道线一般使用三阶或五阶的螺旋线，三阶车道线的表示： \\[y=a_0+a_1*x+a_2*x^2+a_3*x^3\\] 车道线拟合实际上是根据m个采样点\\((x_1,y_1),(x_2,y_2)...(x_m,y_m)\\),拟合出一条n阶多项式曲线方程： \\[y(x,A)=a_0+a_1*x+a_2*x^2+a_3*x^3+...+a_n*x^n=\\sum_{i=0}^{n}{a_i*x^i}\\] 令 \\[A=\\begin{bmatrix} a_0\\\\ a_1\\\\ \\vdots\\\\ a_m \\end{bmatrix}, X=\\begin{bmatrix} 1 &amp; x_1 &amp; x_1^2 &amp; \\cdots &amp; x_1^n\\\\ 1 &amp; x_2 &amp; x_2^2 &amp; \\cdots &amp; x_2^n\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 1 &amp; x_m &amp; x_m^2 &amp; \\cdots &amp; x_m^n \\end{bmatrix} \\] 多项式可表示为： \\[y(x,A)=xA\\] 通过设计cost funciton来评价拟合函数质量，让每个样本点的预测值与目标值(观测值）\\(t_m\\)之间的误差最小，使用均方根误差来评价： \\[E_{RMS}=\\sqrt{\\frac{E(A^*)}{M}}\\] --- - 最小二乘法 误差函数为一元二次，其导数为0时可直接求得方程的唯一解，即解析解（不带正则项的解析解）。 误差函数为： \\[E(A)=\\frac 1 2 \\sum_{m=1}^{M}{(y(x_m,A)-t_m)^2} \\\\ =\\frac 1 2 (XA-T)^{\\top}(XA-T)\\] 对其求导： \\[\\frac{\\partial E(A)}{\\partial A}=X^{\\top}XA-X^{\\top}T\\] 令导数等于0，整理可得： \\[A=(X^{\\top}X)^{-1}X^{\\top}T\\]","categories":[],"tags":[]},{"title":"MNSIT","slug":"hello","date":"2022-03-21T03:58:42.000Z","updated":"2024-11-28T14:56:47.521Z","comments":true,"path":"2022/03/21/hello/","link":"","permalink":"http://example.com/2022/03/21/hello/","excerpt":"","text":"欢迎来到Larry3301的个人博客，多多指教","categories":[],"tags":[]}],"categories":[],"tags":[]}